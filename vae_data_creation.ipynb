{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been used to create sparse training, testing and validation datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def save_weights_pkl(fname, weights):\n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(weights, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_weights_pkl(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    return weights\n",
    "\n",
    "\n",
    "rating_file = 'ml-20m/ratings.csv'\n",
    "out_data_dir = 'ml_dataset'\n",
    "\n",
    "movie_emb_f = 'movie_emb_multilingual.pkl'\n",
    "movie_emb_path = 'ml_dataset/'\n",
    "new_movie_emb_path = movie_emb_path +'movie_emb_multilingual.dic'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(out_data_dir, exist_ok=True)\n",
    "raw_data_orig = pd.read_csv(rating_file, sep=',', header=0)\n",
    "\n",
    "# you are setting the value of movieid to its index after dropping duplicates, seems unnessecary\n",
    "new_movie_id = raw_data_orig['movieId'].drop_duplicates().reset_index().rename(columns={\"index\": \"new_movieId\"})\n",
    "new_user_id = raw_data_orig['userId'].drop_duplicates().reset_index().rename(columns={\"index\": \"new_userId\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_movieId</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26739</th>\n",
       "      <td>19975144</td>\n",
       "      <td>121017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26740</th>\n",
       "      <td>19975145</td>\n",
       "      <td>121019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26741</th>\n",
       "      <td>19975146</td>\n",
       "      <td>121021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26742</th>\n",
       "      <td>19989073</td>\n",
       "      <td>110167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26743</th>\n",
       "      <td>19989075</td>\n",
       "      <td>110510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26744 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       new_movieId  movieId\n",
       "0                0        2\n",
       "1                1       29\n",
       "2                2       32\n",
       "3                3       47\n",
       "4                4       50\n",
       "...            ...      ...\n",
       "26739     19975144   121017\n",
       "26740     19975145   121019\n",
       "26741     19975146   121021\n",
       "26742     19989073   110167\n",
       "26743     19989075   110510\n",
       "\n",
       "[26744 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating  userId  movieId\n",
       "0     3.5       0        0\n",
       "1     3.5       0        1\n",
       "2     3.5       0        2\n",
       "3     3.5       0        3\n",
       "4     3.5       0        4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_full = raw_data_orig.merge(new_user_id, on='userId', how='left').merge(new_movie_id, on='movieId', how='left')\n",
    "movie_id_map = raw_data_full[['movieId','new_movieId' ]].drop_duplicates() # create a map between new and old movieId\n",
    "user_id_map = raw_data_full[['userId','new_userId' ]].drop_duplicates() # create a map between new and old movieId\n",
    "raw_data_full = raw_data_full.drop(labels=['userId', 'movieId', 'timestamp'], axis=1).rename(columns={\"new_userId\": \"userId\", \"new_movieId\": \"movieId\"})\n",
    "raw_data_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=True)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users.\n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "\n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "\n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')\n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 20000263 watching events from 138493 users and 26744 movies (sparsity: 0.540%)\n"
     ]
    }
   ],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data_full, min_uc=5, min_sc=0)\n",
    "\n",
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] *\n",
    "item_popularity.shape[0])\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" %\n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))\n",
    "\n",
    "unique_uid = user_activity.index\n",
    "\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm] # shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train/validation/test users\n",
    "n_users = unique_uid.size\n",
    "n_heldout_users = 10000\n",
    "\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]\n",
    "\n",
    "train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n",
    "\n",
    "raw_data[\"orig_index\"] = raw_data.index.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only using movie ids found in training, not cold start users\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid = pd.unique(train_plays['movieId'])\n",
    "unique_sid_df = pd.DataFrame(unique_sid, columns=[\"movieId\"])\n",
    "unique_sid_df[\"sid\"] = range(unique_sid_df.shape[0])\n",
    "unique_uid_df = pd.DataFrame(unique_uid, columns=[\"userId\"]) # userid, uid\n",
    "unique_uid_df[\"uid\"] = range(unique_uid_df.shape[0])\n",
    "\n",
    "with open(os.path.join(out_data_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)\n",
    "    \n",
    "latest_movie_id = unique_sid_df.rename(columns={\"movieId\": \"new_movieId\"})\n",
    "map_bwteen_3_and_1 = latest_movie_id.merge(movie_id_map, on='new_movieId')[[\"sid\", \"movieId\"]]\n",
    "map_bwteen_3_and_1.index = map_bwteen_3_and_1[\"movieId\"]\n",
    "reverse = dict(map_bwteen_3_and_1['sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ml-20m/mapping_ml.pkl', 'wb') as f:\n",
    "    pickle.dump(reverse, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use movie embeddings generated from MP net to encode users, map using the new ids created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_filpath = movie_emb_path + movie_emb_f\n",
    "with open(emb_filpath, 'rb') as f:\n",
    "    movie_embedding_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "new_movie_embedding_dict = {reverse[k]:v for k,v in movie_embedding_dict.items() if k in reverse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26163"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(new_movie_embedding_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eb9af3d4f37b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create a dictionary to store the encoded embeddings for users\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ml-20m/movie_emb_multilingual.dic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_movie_embedding_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create a dictionary to store the encoded embeddings for users\n",
    "with open(new_movie_emb, 'wb') as f:\n",
    "    pickle.dump(new_movie_embedding_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group) # n records for this user\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool') # array([False, False, False])\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list) # data frame containinga ll columns not just the item id\n",
    "    data_te = pd.concat(te_list)\n",
    "\n",
    "    return data_tr, data_te\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n",
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "7000 users sampled\n",
      "8000 users sampled\n",
      "9000 users sampled\n"
     ]
    }
   ],
   "source": [
    "train_plays = train_plays.merge(unique_uid_df, on='userId', how='left')\n",
    "train_plays = train_plays.merge(unique_sid_df, on='movieId', how='left')\n",
    "train_plays[\"uid_fm0\"] = train_plays[\"uid\"] # add the same column as valid and test dataset\n",
    "\n",
    "train_plays_profile = train_plays.drop_duplicates(subset=\"uid_fm0\").filter(regex=\"^[ugac].*\") # unique data for each user\n",
    "assert train_plays_profile['uid'].shape[0] == n_users - n_heldout_users * 2\n",
    "\n",
    "vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]\n",
    "\n",
    "vad_plays = vad_plays.merge(unique_uid_df, on='userId', how='left')\n",
    "vad_plays = vad_plays.merge(unique_sid_df, on='movieId', how='left')\n",
    "\n",
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
    "vad_plays_tr.reset_index(drop=True, inplace=True)\n",
    "vad_plays_te.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]\n",
    "\n",
    "test_plays = test_plays.merge(unique_uid_df, on='userId', how='left')\n",
    "test_plays = test_plays.merge(unique_sid_df, on='movieId', how='left')\n",
    "\n",
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
    "test_plays_tr.reset_index(drop=True, inplace=True)\n",
    "test_plays_te.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_data = train_plays.filter(items=[\"uid\", \"sid\"], axis=1)\n",
    "train_data.to_csv(os.path.join(out_data_dir, 'train.csv'), index=False)\n",
    "\n",
    "vad_data_tr = vad_plays_tr.filter(items=[\"uid\", \"sid\"], axis=1)\n",
    "vad_data_tr.to_csv(os.path.join(out_data_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "vad_data_te = vad_plays_te.filter(items=[\"uid\", \"sid\"], axis=1)\n",
    "vad_data_te.to_csv(os.path.join(out_data_dir, 'validation_te.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = min(vad_plays_tr['uid'].min(), vad_plays_te['uid'].min())\n",
    "end_idx = max(vad_plays_tr['uid'].max(), vad_plays_te['uid'].max())\n",
    "vad_plays_tr['uid_fm0'] = vad_plays_tr['uid'] - start_idx\n",
    "vad_plays_te['uid_fm0'] = vad_plays_te['uid'] - start_idx\n",
    "\n",
    "test_data_tr = test_plays_tr.filter(items=[\"uid\", \"sid\"], axis=1)\n",
    "test_data_tr.to_csv(os.path.join(out_data_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_data_te = test_plays_te.filter(items=[\"uid\", \"sid\"], axis=1)\n",
    "test_data_te.to_csv(os.path.join(out_data_dir, 'test_te.csv'), index=False)\n",
    "\n",
    "start_idx = min(test_plays_tr['uid'].min(), test_plays_te['uid'].min())\n",
    "end_idx = max(test_plays_tr['uid'].max(), test_plays_te['uid'].max())\n",
    "test_plays_tr['uid_fm0'] = test_plays_tr['uid'] - start_idx\n",
    "test_plays_te['uid_fm0'] = test_plays_te['uid'] - start_idx\n",
    "\n",
    "unique_sid = list()\n",
    "with open(os.path.join(out_data_dir, 'unique_sid.txt'), 'r') as f:\n",
    "    for line in f:\n",
    "        unique_sid.append(line.strip())\n",
    "\n",
    "n_items = len(unique_sid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create sparse datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(csv_file):\n",
    "    tp = pd.read_csv(csv_file)\n",
    "    n_users = tp['uid'].max() + 1\n",
    "\n",
    "    rows, cols = tp['uid'], tp['sid']\n",
    "    data = sparse.csr_matrix((np.ones_like(rows), (rows, cols)), dtype='float64', shape=(n_users, n_items))\n",
    "    return data\n",
    "\n",
    "train_data_csr = load_train_data(os.path.join(out_data_dir, 'train.csv'))\n",
    "\n",
    "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
    "    tp_tr = pd.read_csv(csv_file_tr)\n",
    "    tp_te = pd.read_csv(csv_file_te)\n",
    "\n",
    "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "    assert pd.unique(tp_tr[\"uid\"]).shape[0] == end_idx - start_idx + 1\n",
    "    assert pd.unique(tp_te[\"uid\"]).shape[0] == end_idx - start_idx + 1\n",
    "\n",
    "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr), (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    data_te = sparse.csr_matrix((np.ones_like(rows_te), (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
    "    return data_tr, data_te\n",
    "\n",
    "vad_data_tr_csr, vad_data_te_csr = load_tr_te_data(os.path.join(out_data_dir, 'validation_tr.csv'), os.path.join(out_data_dir, 'validation_te.csv'))\n",
    "\n",
    "test_data_tr_csr, test_data_te_csr = load_tr_te_data(os.path.join(out_data_dir, 'test_tr.csv'), os.path.join(out_data_dir, 'test_te.csv'))\n",
    "\n",
    "fname = os.path.join(out_data_dir, 'data_csr.pkl')\n",
    "datas = [train_data_csr, vad_data_tr_csr, vad_data_te_csr, test_data_tr_csr, test_data_te_csr]\n",
    "save_weights_pkl(fname, datas)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
